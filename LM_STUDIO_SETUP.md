# üéõÔ∏è –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LM Studio

–≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ MCP —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LM Studio –≤–º–µ—Å—Ç–æ Ollama.

## üìã –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LM Studio

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LM Studio

1. **–°–∫–∞—á–∞–π—Ç–µ LM Studio** —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞: https://lmstudio.ai/
2. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ** LM Studio
3. **–°–∫–∞—á–∞–π—Ç–µ GPT OSS 20B –º–æ–¥–µ–ª—å:**
   - –û—Ç–∫—Ä–æ–π—Ç–µ –≤–∫–ª–∞–¥–∫—É "Search"
   - –ù–∞–π–¥–∏—Ç–µ "openai/gpt-oss-20b"
   - –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å (—ç—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è)

### 2. –ó–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞

–í LM Studio:
1. **–ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤–æ –≤–∫–ª–∞–¥–∫—É "Local Server"**
2. **–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å** "openai/gpt-oss-20b"
3. **–ù–∞–∂–º–∏—Ç–µ "Start Server"**
4. **–£–±–µ–¥–∏—Ç–µ—Å—å**, —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ `http://localhost:1234`

### 3. –û–±–Ω–æ–≤–∏—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é MCP —Å–µ—Ä–≤–µ—Ä–∞

–û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª `config/server_config.json` –∏ –∏–∑–º–µ–Ω–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:

```json
{
  "server": {
    "name": "autonomous-gpt-oss-20b-lmstudio",
    "version": "1.0.0",
    "host": "localhost",
    "port": 8080,
    "max_connections": 10
  },
  "gpt_oss": {
    "model_name": "openai/gpt-oss-20b",
    "base_url": "http://localhost:1234",
    "api_endpoint": "/v1/chat/completions",
    "max_tokens": 4000,
    "temperature": 0.7,
    "reasoning_level": "medium"
  },
  "lm_studio": {
    "enabled": true,
    "local_server_url": "http://localhost:1234",
    "model_loaded_check": true,
    "timeout": 60
  },
  "monitoring": {
    "update_interval": 5,
    "log_level": "INFO",
    "max_log_files": 10,
    "emotion_update_rate": 10
  },
  "database": {
    "path": "data/autonomous_tasks.db",
    "backup_interval": 3600,
    "max_task_history": 1000
  }
}
```

### 4. –°–æ–∑–¥–∞–π—Ç–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ –¥–ª—è LM Studio

Windows (`start_server_lmstudio.bat`):
```batch
@echo off
echo ================================================================
echo     Autonomous GPT OSS 20B MCP Server - LM Studio Version
echo ================================================================
echo.

REM –ü—Ä–æ–≤–µ—Ä–∫–∞ Python
echo [1/4] Checking Python installation...
python --version >nul 2>&1
if errorlevel 1 (
    echo [ERROR] Python not found! Please install Python 3.8+
    pause
    exit /b 1
)
echo [OK] Python found

REM –ü—Ä–æ–≤–µ—Ä–∫–∞ LM Studio API
echo [2/4] Checking LM Studio server...
curl -s http://localhost:1234/v1/models >nul 2>&1
if errorlevel 1 (
    echo [ERROR] LM Studio server not responding!
    echo Please ensure:
    echo 1. LM Studio is running
    echo 2. GPT OSS 20B model is loaded
    echo 3. Local server is started on port 1234
    pause
    exit /b 1
)
echo [OK] LM Studio server is running

REM –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π Python
echo [3/4] Installing Python dependencies...
pip install --quiet psutil requests asyncio
if errorlevel 1 (
    echo [WARNING] Some packages may already be installed
)

REM –ü—Ä–æ–≤–µ—Ä–∫–∞ MCP
python -c "import mcp" 2>nul
if errorlevel 1 (
    echo Installing MCP library...
    pip install mcp
)

echo [4/4] Checking GPT OSS 20B model availability...
curl -s -X POST http://localhost:1234/v1/chat/completions -H "Content-Type: application/json" -d "{\"model\":\"gpt-oss-20b\",\"messages\":[{\"role\":\"user\",\"content\":\"test\"}],\"max_tokens\":5}" >nul 2>&1
if errorlevel 1 (
    echo [WARNING] GPT OSS 20B model may not be properly loaded
    echo Please check LM Studio and ensure the model is loaded
)

echo.
echo ================================================================
echo                    STARTING MCP SERVER
echo ================================================================
echo.
echo üéõÔ∏è Using LM Studio instead of Ollama
echo üéÆ Control Center GUI will open automatically
echo üé≠ Emotional Display window will appear
echo üö® DO NOT enable unrestricted access unless testing!
echo.
echo Press Ctrl+C to stop the server
echo.

REM –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è LM Studio
set LM_STUDIO_MODE=1

REM –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞
python main.py

pause
```

Linux (`start_server_lmstudio.sh`):
```bash
#!/bin/bash

echo "================================================================"
echo "     Autonomous GPT OSS 20B MCP Server - LM Studio Version"
echo "================================================================"
echo

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Python
echo "[1/4] Checking Python installation..."
if ! command -v python3 &> /dev/null; then
    echo "[ERROR] Python3 not found!"
    exit 1
fi
echo "[OK] Python found"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ LM Studio API
echo "[2/4] Checking LM Studio server..."
if ! curl -s http://localhost:1234/v1/models > /dev/null 2>&1; then
    echo "[ERROR] LM Studio server not responding!"
    echo "Please ensure:"
    echo "1. LM Studio is running"
    echo "2. GPT OSS 20B model is loaded"
    echo "3. Local server is started on port 1234"
    exit 1
fi
echo "[OK] LM Studio server is running"

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π Python
echo "[3/4] Installing Python dependencies..."
pip3 install --quiet psutil requests asyncio

# –ü—Ä–æ–≤–µ—Ä–∫–∞ MCP
if ! python3 -c "import mcp" 2>/dev/null; then
    echo "Installing MCP library..."
    pip3 install mcp
fi

echo "[4/4] Checking GPT OSS 20B model availability..."
if ! curl -s -X POST http://localhost:1234/v1/chat/completions \
     -H "Content-Type: application/json" \
     -d '{"model":"gpt-oss-20b","messages":[{"role":"user","content":"test"}],"max_tokens":5}' > /dev/null 2>&1; then
    echo "[WARNING] GPT OSS 20B model may not be properly loaded"
    echo "Please check LM Studio and ensure the model is loaded"
fi

echo
echo "================================================================"
echo "                    STARTING MCP SERVER"
echo "================================================================"
echo
echo "üéõÔ∏è Using LM Studio instead of Ollama"
echo "üéÆ Control Center GUI will open automatically"
echo "üé≠ Emotional Display window will appear"  
echo "üö® DO NOT enable unrestricted access unless testing!"
echo
echo "Press Ctrl+C to stop the server"
echo

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è LM Studio
export LM_STUDIO_MODE=1

# –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞
python3 main.py
```

### 5. –û–±–Ω–æ–≤–∏—Ç–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ LM Studio

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `lm_studio_adapter.py`:

```python
import os
import requests
import json
from typing import Dict, Any, List

class LMStudioAdapter:
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LM Studio API"""
    
    def __init__(self, base_url="http://localhost:1234"):
        self.base_url = base_url
        self.api_endpoint = "/v1/chat/completions"
        self.models_endpoint = "/v1/models"
        
    def check_server_status(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ LM Studio —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            response = requests.get(f"{self.base_url}{self.models_endpoint}", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_available_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            response = requests.get(f"{self.base_url}{self.models_endpoint}", timeout=5)
            if response.status_code == 200:
                models_data = response.json()
                return [model["id"] for model in models_data.get("data", [])]
            return []
        except:
            return []
    
    def send_chat_request(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LM Studio"""
        payload = {
            "model": kwargs.get("model", "gpt-oss-20b"),
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", 4000),
            "temperature": kwargs.get("temperature", 0.7),
            "stream": kwargs.get("stream", False)
        }
        
        try:
            response = requests.post(
                f"{self.base_url}{self.api_endpoint}",
                json=payload,
                timeout=kwargs.get("timeout", 60)
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                return {
                    "error": f"LM Studio API error: {response.status_code}",
                    "details": response.text
                }
        except Exception as e:
            return {
                "error": f"Connection error: {str(e)}"
            }
    
    def test_model_response(self, model_name="gpt-oss-20b") -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏"""
        test_messages = [
            {"role": "user", "content": "Hello! Can you confirm you're GPT OSS 20B?"}
        ]
        
        return self.send_chat_request(test_messages, model=model_name, max_tokens=100)


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ä–µ–∂–∏–º–∞
def get_llm_backend():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –±—ç–∫–µ–Ω–¥–∞"""
    lm_studio = LMStudioAdapter()
    
    if os.getenv("LM_STUDIO_MODE") == "1" or lm_studio.check_server_status():
        return "lm_studio", lm_studio
    else:
        return "ollama", None
```

### 6. –û–±–Ω–æ–≤–∏—Ç–µ –º–æ–¥—É–ª—å –¥–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è LM Studio

–í —Ñ–∞–π–ª–µ `finetuning.py` –Ω–∞–π–¥–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é `generate_example_variations()` –∏ –∑–∞–º–µ–Ω–∏—Ç–µ –≤—ã–∑–æ–≤ Ollama API:

```python
def generate_example_variations(self):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ —á–µ—Ä–µ–∑ GPT OSS 20B (LM Studio)"""
    input_text = self.input_text.get('1.0', tk.END).strip()
    
    if not input_text:
        messagebox.showwarning("Warning", "Please provide input text to generate variations")
        return
    
    self.update_status("Generating variations...")
    
    def generate_variations():
        try:
            from lm_studio_adapter import LMStudioAdapter
            
            lm_studio = LMStudioAdapter()
            
            if not lm_studio.check_server_status():
                self.window.after(0, lambda: self.update_status("LM Studio server not available"))
                return
            
            variations_prompt = f"""Generate 3 similar but different training examples based on this input:

Input: {input_text}

Create variations that:
1. Use different wording but same intent
2. Include edge cases or different parameters
3. Show different levels of detail in responses

Format as JSON array with objects containing 'input' and 'output' fields."""
            
            messages = [{"role": "user", "content": variations_prompt}]
            response = lm_studio.send_chat_request(messages, temperature=0.8)
            
            if "error" not in response:
                variations_text = response["choices"][0]["message"]["content"]
                
                # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
                try:
                    json_match = re.search(r'\[.*\]', variations_text, re.DOTALL)
                    if json_match:
                        variations_data = json.loads(json_match.group())
                        
                        cursor = self.training_db.cursor()
                        added_count = 0
                        
                        for variation in variations_data:
                            if isinstance(variation, dict) and 'input' in variation and 'output' in variation:
                                cursor.execute(
                                    "INSERT INTO training_examples (category, input_text, expected_output) VALUES (?, ?, ?)",
                                    (self.category_var.get(), variation['input'], variation['output'])
                                )
                                added_count += 1
                        
                        self.training_db.commit()
                        self.window.after(0, lambda: self.update_status(f"Generated {added_count} variations"))
                        self.window.after(0, self.update_preview)
                        self.window.after(0, self.refresh_library)
                        
                    else:
                        self.window.after(0, lambda: self.update_status("Failed to parse variations"))
                        
                except json.JSONDecodeError:
                    self.window.after(0, lambda: self.update_status("Failed to parse variations as JSON"))
                    
            else:
                self.window.after(0, lambda: self.update_status(f"LM Studio error: {response['error']}"))
                
        except Exception as e:
            self.window.after(0, lambda: self.update_status(f"Error: {str(e)}"))
    
    threading.Thread(target=generate_variations, daemon=True).start()
```

## üéØ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ LM Studio

**–ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Ollama:**
- ‚úÖ **GUI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** - —É–¥–æ–±–Ω–µ–µ —É–ø—Ä–∞–≤–ª—è—Ç—å –º–æ–¥–µ–ª—è–º–∏
- ‚úÖ **–õ—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö  
- ‚úÖ **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ GPU** —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
- ‚úÖ **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤** –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π
- ‚úÖ **–ü—Ä–æ—Å—Ç–∞—è —Å–º–µ–Ω–∞ –º–æ–¥–µ–ª–µ–π** —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

## üöÄ –ó–∞–ø—É—Å–∫ —Å LM Studio

1. **–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ LM Studio –∑–∞–ø—É—â–µ–Ω** —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é GPT OSS 20B
2. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ –Ω–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç:**
   ```bash
   # Windows
   start_server_lmstudio.bat
   
   # Linux  
   ./start_server_lmstudio.sh
   ```
3. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ** - –≤ –ª–æ–≥–∞—Ö –¥–æ–ª–∂–Ω–æ –ø–æ—è–≤–∏—Ç—å—Å—è "Using LM Studio backend"

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–í LM Studio –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ:
- **GPU Layers**: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–π GPU
- **Context Length**: 4096 –∏–ª–∏ 8192 –¥–ª—è –ª—É—á—à–µ–π —Ä–∞–±–æ—Ç—ã
- **Batch Size**: –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –ø–æ–¥ –≤–∞—à—É —Å–∏—Å—Ç–µ–º—É

---

**–¢–µ–ø–µ—Ä—å –≤–∞—à MCP —Å–µ—Ä–≤–µ—Ä –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å LM Studio –≤–º–µ—Å—Ç–æ Ollama! üéõÔ∏è**